# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TX3AR0BNNTLOjGfB9tAr8B-sj6OY9eGd
"""

import streamlit as st
import pickle
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import torch
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_huggingface import HuggingFacePipeline

# Set page config
st.set_page_config(page_title="PDF AI Assistant", layout="wide")
st.title("üìò AI-Powered PDF Assistant")
st.markdown("Enter your query below and get intelligent responses based on the uploaded document.")

# Load Model and Tokenizer
@st.cache_resource
def load_model():
    model_name = "meta-llama/Llama-3.2-3B"
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map="auto")
    return model, tokenizer

model, tokenizer = load_model()
st.success("Llama 3.2 Model Loaded Successfully!")

# Load and Process PDF
pdf_path = "/content/Candidate-Handbook-February-12-2025-cover-com.pdf"
loader = PyPDFLoader(pdf_path)
documents = loader.load()
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)
docs = text_splitter.split_documents(documents)

# Convert documents into embeddings
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
vector_store = FAISS.from_documents(docs, embedding_model)
vector_store.save_local("faiss_index")
retriever = vector_store.as_retriever(search_kwargs={"k": 10})

# Define LLM pipeline
llm_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    truncation=True,
    do_sample=True,
    temperature=0.7,
    pad_token_id=tokenizer.eos_token_id
)

# Define RAG pipeline
rag_chain = RetrievalQA.from_chain_type(
    llm=HuggingFacePipeline(pipeline=llm_pipeline),
    retriever=retriever
)

# Streamlit Input Box
query = st.text_input("üîç Enter your query:", "What are Consumer Benefits?")

if st.button("Get Response"):
    with st.spinner("Processing..."):
        response = rag_chain.invoke(query)
        formatted_output = response['result'].replace("‚Ä¢", "-").replace("\n\n", "\n").strip()

        # Save response
        with open("formatted_output.pkl", "wb") as f:
            pickle.dump(formatted_output, f)

        st.subheader("üìú Response:")
        st.success(formatted_output)